---
title: 乌云白帽子爬虫
date: 2016-05-18 21:52:21
categories: Python
tags: 爬虫
---

最近写了一个爬取乌云白帽子信息的爬虫。总结一下。

<!--more-->
1、获取所有白帽子主页链接  
首先循环遍历，得到所有主页链接
```
allurl = []
tempurl = []
for i in range(1, 116):
    i = str(i)
    url = 'http://www.wooyun.org/whitehats/do/1/page/' + i
    html = spider.get(url, headers=header)
    soup = BeautifulSoup(html.text, 'html.parser')
    links = soup.find_all('a', href=re.compile("/whitehats/.*"))
    for link in links:
        ahref = 'http://www.wooyun.org' + link.get('href')
        tempurl.append(ahref)
for element in tempurl:
    if ('http://www.wooyun.org/whitehats/do/' not in element) and ('http://www.wooyun.org/whitehats/' != element):
        allurl.append(element)
return allurl
```

2、得到所有数据  
根据获取的主页链接，循环解析，获取rank,个人主页。
此处写的有点烂。感觉正则写的太差了。
```
for url in allurl:
    html = spider.get(url, headers=header)
    soup = BeautifulSoup(html.text, 'html.parser')
    for tag in soup.find_all('ul', class_="time"):
        string = tag.get_text(strip=True)
    re_hat_level = re.search("\w{2}白帽子", string)
        all_hat_level.append(re_hat_level.group())
        re_rank_num = re.search("Rank:\s\d+", string)
        rank_temp = re_rank_num.group()
        rank_temp = int(rank_temp[6:])
        all_hat_rank.append(rank_temp)
        re_page = str(soup.find_all("a", rel="nofollow"))
        re_hat_page = re.search("href=\".+?rel", re_page, re.IGNORECASE)
        try:
            re_hat_page = re_hat_page.group()[6:-5]
        except:
            # print('错误页面:', url)
            re_hat_page = '没有获得主页'
        all_hat_page.append(re_hat_page)
```
3、输出到文件  
将得到的数据按照rank高低排序。然后输出。
创建一个html文件
```
for i in range(0, len(allurl)):
    fout.write("<tr>")
    fout.write("<td>%d</td>" % i)
    fout.write("<td><a href=%s target=\"_blank\">%s</a>&nbsp;</td>" %
               (allurl[i], allurl[i]))
    fout.write("<td>Rank: %s &nbsp;</td>" % all_hat_level[i])
    fout.write("<td>%s &nbsp;&nbsp;</td>" % all_hat_rank[i])
    fout.write("<td><a href=%s target=\"_blank\">%s</a></td>" %
               (all_hat_page[i], all_hat_page[i]))
    fout.write("</tr>")
```
4、最后结果如图
![](http://7xo8y2.com1.z0.glb.clouddn.com/wyspider2016-05-19%2018-32-56%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png)


5、问题   
爬虫还是有几个问题。主要时爬取速度太慢，以及正则解析还是有点渣。
